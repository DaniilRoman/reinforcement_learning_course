{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "1_reinforce.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZE3z6eho-1O",
        "colab_type": "text"
      },
      "source": [
        "# REINFORCE in pytorch (5 pts)\n",
        "\n",
        "Just like we did before for q-learning, this time we'll design a pytorch network to learn `CartPole-v0` via policy gradient (REINFORCE).\n",
        "\n",
        "Most of the code in this notebook is taken from approximate qlearning, so you'll find it more or less familiar and even simpler."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c55HtkdPo-1P",
        "colab_type": "code",
        "outputId": "5d1504ea-7c74-4b34-994d-c348e8f742e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "import sys, os\n",
        "if 'google.colab' in sys.modules and not os.path.exists('.setup_complete'):\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/spring20/setup_colab.sh -O- | bash\n",
        "    !touch .setup_complete\n",
        "\n",
        "# This code creates a virtual display to draw game images on.\n",
        "# It will have no effect if your machine has a monitor.\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
        "    !bash ../xvfb start\n",
        "    os.environ['DISPLAY'] = ':1'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Selecting previously unselected package xvfb.\n",
            "(Reading database ... 144568 files and directories currently installed.)\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.4_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.4) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.4) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Starting virtual X frame buffer: Xvfb.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRGf-hkVo-1V",
        "colab_type": "code",
        "outputId": "b3648d9f-9682-4350-9646-a5180841d75f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "env = gym.make(\"CartPole-v0\").env\n",
        "env.reset()\n",
        "\n",
        "plt.imshow(env.render(\"rgb_array\"))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f91ed451b70>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNAxmCY9o-1a",
        "colab_type": "text"
      },
      "source": [
        "# Building the network for REINFORCE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AW63pY19o-1b",
        "colab_type": "text"
      },
      "source": [
        "For REINFORCE algorithm, we'll need a model that predicts action probabilities given states. Let's define such a model below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lWvt2alo-1c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PVb8fgXqqn0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "state_dim = env.observation_space.shape[0]\n",
        "n_actions = env.action_space.n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GueWNte-o-1j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build a simple neural network that predicts policy logits. \n",
        "# Keep it simple: CartPole isn't worth deep architectures.\n",
        "model = nn.Sequential(\n",
        "          nn.Linear(state_dim, 100),\n",
        "          nn.ReLU(inplace = True),\n",
        "          nn.Linear(100, n_actions)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OAM243qo-1n",
        "colab_type": "text"
      },
      "source": [
        "#### Predict function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3FTv3i3o-1o",
        "colab_type": "text"
      },
      "source": [
        "Note: output value of this function is not a torch tensor, it's a numpy array.\n",
        "So, here gradient calculation is not needed.\n",
        "<br>\n",
        "Use [no_grad](https://pytorch.org/docs/stable/autograd.html#torch.autograd.no_grad)\n",
        "to suppress gradient calculation.\n",
        "<br>\n",
        "Also, `.detach()` (or legacy `.data` property) can be used instead, but there is a difference:\n",
        "<br>\n",
        "With `.detach()` computational graph is built but then disconnected from a particular tensor,\n",
        "so `.detach()` should be used if that graph is needed for backprop via some other (not detached) tensor;\n",
        "<br>\n",
        "In contrast, no graph is built by any operation in `no_grad()` context, thus it's preferable here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkYL-A8oGye3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.autograd import Variable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oal5DssMo-1p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_probs(states):\n",
        "    \"\"\" \n",
        "    Predict action probabilities given states.\n",
        "    :param states: numpy array of shape [batch, state_shape]\n",
        "    :returns: numpy array of shape [batch, n_actions]\n",
        "    \"\"\"\n",
        "    # convert states, compute logits, use softmax to get probability\n",
        "    # <YOUR CODE>\n",
        "    states = Variable(torch.FloatTensor(states))\n",
        "    probs = nn.functional.softmax(model.forward(states))\n",
        "\n",
        "    return probs.data.numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZPN6SDPo-1u",
        "colab_type": "code",
        "outputId": "b6c824e3-8387-40a5-b0e3-f37429cdfbd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "test_states = np.array([env.reset() for _ in range(5)])\n",
        "test_probas = predict_probs(test_states)\n",
        "assert isinstance(\n",
        "    test_probas, np.ndarray), \"you must return np array and not %s\" % type(test_probas)\n",
        "assert tuple(test_probas.shape) == (\n",
        "    test_states.shape[0], env.action_space.n), \"wrong output shape: %s\" % np.shape(test_probas)\n",
        "assert np.allclose(np.sum(test_probas, axis=1),\n",
        "                   1), \"probabilities do not sum to 1\""
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74fVusSxo-1z",
        "colab_type": "text"
      },
      "source": [
        "### Play the game\n",
        "\n",
        "We can now use our newly built agent to play the game."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "299eXdR-o-10",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_session(env, t_max=1000):\n",
        "    \"\"\" \n",
        "    play a full session with REINFORCE agent and train at the session end.\n",
        "    returns sequences of states, actions and rewards\n",
        "    \"\"\"\n",
        "    # arrays to record session\n",
        "    states, actions, rewards = [], [], []\n",
        "    s = env.reset()\n",
        "\n",
        "    for t in range(t_max):\n",
        "        # action probabilities array aka pi(a|s)\n",
        "        action_probs = predict_probs(np.array([s]))[0]\n",
        "\n",
        "        # Sample action with given probabilities.\n",
        "        # a = <YOUR CODE>\n",
        "        a = np.random.choice(n_actions, p=action_probs)\n",
        "        new_s, r, done, info = env.step(a)\n",
        "\n",
        "        # record session history to train later\n",
        "        states.append(s)\n",
        "        actions.append(a)\n",
        "        rewards.append(r)\n",
        "\n",
        "        s = new_s\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    return states, actions, rewards"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFLQONwVo-14",
        "colab_type": "code",
        "outputId": "6c2c088a-b50d-420d-b719-df83ea1602ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# test it\n",
        "states, actions, rewards = generate_session(env)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WH9bD0uco-18",
        "colab_type": "text"
      },
      "source": [
        "### Computing cumulative rewards"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFH6Xurmo-19",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_cumulative_rewards(rewards,  # rewards at each step\n",
        "                           gamma=0.99  # discount for reward\n",
        "                           ):\n",
        "    \"\"\"\n",
        "    take a list of immediate rewards r(s,a) for the whole session \n",
        "    compute cumulative returns (a.k.a. G(s,a) in Sutton '16)\n",
        "    G_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n",
        "\n",
        "    The simple way to compute cumulative rewards is to iterate from last to first time tick\n",
        "    and compute G_t = r_t + gamma*G_{t+1} recurrently\n",
        "\n",
        "    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n",
        "    \"\"\"\n",
        "    # return <YOUR CODE: array of cumulative rewards>\n",
        "    cumulative_rewards = []\n",
        "    prev = 0.\n",
        "    for r in reversed(rewards):\n",
        "        prev = r + gamma * prev\n",
        "        cumulative_rewards.append(prev)\n",
        "    cumulative_rewards.reverse()\n",
        "    return cumulative_rewards"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lPGvI8Qo-2B",
        "colab_type": "code",
        "outputId": "9f15a2be-35f6-4bf3-a1d4-37080bca9f59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "get_cumulative_rewards(rewards)\n",
        "assert len(get_cumulative_rewards(list(range(100)))) == 100\n",
        "assert np.allclose(get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9), [\n",
        "                   1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n",
        "assert np.allclose(get_cumulative_rewards(\n",
        "    [0, 0, 1, -2, 3, -4, 0], gamma=0.5), [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n",
        "assert np.allclose(get_cumulative_rewards(\n",
        "    [0, 0, 1, 2, 3, 4, 0], gamma=0), [0, 0, 1, 2, 3, 4, 0])\n",
        "print(\"looks good!\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "looks good!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qz8WODe2o-2F",
        "colab_type": "text"
      },
      "source": [
        "#### Loss function and updates\n",
        "\n",
        "We now need to define objective and update over policy gradient.\n",
        "\n",
        "Our objective function is\n",
        "\n",
        "$$ J \\approx  { 1 \\over T } \\sum_{i=1}^T  G(s_i,a_i) $$\n",
        "\n",
        "\n",
        "Following the REINFORCE algorithm, we can define our objective as follows: \n",
        "\n",
        "$$ \\hat J \\approx { 1 \\over T } \\sum_{i=1}^T \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G(s_i,a_i) $$\n",
        "\n",
        "Entropy Regularizer\n",
        "  $$ H = - {1 \\over T} \\sum_{i=1}^T  \\sum_{a \\in A} {\\pi_\\theta(a|s_i) \\cdot \\log \\pi_\\theta(a|s_i)}$$\n",
        "\n",
        "$T$ is session length\n",
        "\n",
        "So we optimize a linear combination of $- \\hat J$, $-H$\n",
        "\n",
        "When you compute gradient of that function over network weights $ \\theta $, it will become exactly the policy gradient."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQoJ4mh0o-2G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def to_one_hot(y_tensor, ndims):\n",
        "    \"\"\" helper: take an integer vector and convert it to 1-hot matrix. \"\"\"\n",
        "    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n",
        "    y_one_hot = torch.zeros(\n",
        "        y_tensor.size()[0], ndims).scatter_(1, y_tensor, 1)\n",
        "    return y_one_hot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWobLztLo-2L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your code: define optimizers\n",
        "# optimizer = <YOUR CODE>\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "\n",
        "def train_on_session(states, actions, rewards, gamma=0.99, entropy_coef=1e-2):\n",
        "    \"\"\"\n",
        "    Takes a sequence of states, actions and rewards produced by generate_session.\n",
        "    Updates agent's weights by following the policy gradient above.\n",
        "    Please use Adam optimizer with default parameters.\n",
        "    \"\"\"\n",
        "\n",
        "    # cast everything into torch tensors\n",
        "    states = torch.tensor(states, dtype=torch.float32)\n",
        "    actions = torch.tensor(actions, dtype=torch.int32)\n",
        "    cumulative_returns = np.array(get_cumulative_rewards(rewards, gamma))\n",
        "    cumulative_returns = torch.tensor(cumulative_returns, dtype=torch.float32)\n",
        "\n",
        "    # predict logits, probas and log-probas using an agent.\n",
        "    logits = model(states)\n",
        "    probs = nn.functional.softmax(logits, -1)\n",
        "    log_probs = nn.functional.log_softmax(logits, -1)\n",
        "\n",
        "    assert all(isinstance(v, torch.Tensor) for v in [logits, probs, log_probs]), \\\n",
        "        \"please use compute using torch tensors and don't use predict_probs function\"\n",
        "\n",
        "    # select log-probabilities for chosen actions, log pi(a_i|s_i)\n",
        "    log_probs_for_actions = torch.sum(\n",
        "        log_probs * to_one_hot(actions, env.action_space.n), dim=1)\n",
        "   \n",
        "    # Compute loss here. Don't forgen entropy regularization with `entropy_coef` \n",
        "    #<YOUR CODE>\n",
        "    J = torch.mean(log_probs_for_actions *cumulative_returns)\n",
        "    entropy = -(probs*log_probs).sum(-1).mean()\n",
        "    loss = - J - entropy_coef * entropy     \n",
        "\n",
        "    # Gradient descent step\n",
        "    # <YOUR CODE>\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # technical: return session rewards to print them later\n",
        "    return np.sum(rewards)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jyf-ZwxIo-2Q",
        "colab_type": "text"
      },
      "source": [
        "### The actual training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omr0eQkRo-2Q",
        "colab_type": "code",
        "outputId": "5e46a414-679a-479e-da4f-342d5413ebff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "for i in range(100):\n",
        "    rewards = [train_on_session(*generate_session(env))\n",
        "               for _ in range(100)]  # generate new sessions\n",
        "    print(\"mean reward:%.3f\" % (np.mean(rewards)))\n",
        "    if np.mean(rewards) > 500:\n",
        "        print(\"You Win!\")  # but you can train even further\n",
        "        break"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "mean reward:18.040\n",
            "mean reward:30.280\n",
            "mean reward:41.430\n",
            "mean reward:42.130\n",
            "mean reward:71.150\n",
            "mean reward:108.460\n",
            "mean reward:130.960\n",
            "mean reward:158.100\n",
            "mean reward:156.540\n",
            "mean reward:372.290\n",
            "mean reward:168.470\n",
            "mean reward:213.730\n",
            "mean reward:419.730\n",
            "mean reward:286.880\n",
            "mean reward:522.840\n",
            "You Win!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bii6SzbGo-2U",
        "colab_type": "text"
      },
      "source": [
        "### Video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfLUxt3_o-2V",
        "colab_type": "code",
        "outputId": "51b0bf73-aa04-4d8c-fea8-f0d6624045b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "# record sessions\n",
        "import gym.wrappers\n",
        "monitor_env = gym.wrappers.Monitor(gym.make(\"CartPole-v0\"),\n",
        "                           directory=\"videos\", force=True)\n",
        "sessions = [generate_session(monitor_env) for _ in range(100)]\n",
        "monitor_env.close()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1f75ymBo-2Y",
        "colab_type": "code",
        "outputId": "89255b8d-dd65-4242-b4bd-9d4fbb25e98c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 501
        }
      },
      "source": [
        "# show video\n",
        "from IPython.display import HTML\n",
        "import os\n",
        "\n",
        "video_names = list(\n",
        "    filter(lambda s: s.endswith(\".mp4\"), os.listdir(\"./videos/\")))\n",
        "\n",
        "HTML(\"\"\"\n",
        "<video width=\"640\" height=\"480\" controls>\n",
        "  <source src=\"{}\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\".format(\"./videos/\" + video_names[-1]))  # this may or may not be the _last_ video. Try other indices"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "<video width=\"640\" height=\"480\" controls>\n",
              "  <source src=\"./videos/openaigym.video.0.123.video000000.mp4\" type=\"video/mp4\">\n",
              "</video>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-XaY25Oo-2d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}